{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETFpFlCmeuSB"
   },
   "source": [
    "# Modele Seq2Seq i atencja\n",
    "Poniższy notebook jest inspirowany tym tutorialem PyTorcha: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html. Zachęcam żeby zajrzeć tam po więcej informacji.\n",
    "\n",
    "W tym notebooku będziemy próbować rozwiązać problem automatycznego tłumaczenia zdań z jednego języka naturalnego na drugi -- konkretniej z języka polskiego na angielski. Dla przykładu model otrzymujący zdanie:\n",
    "\n",
    "> Myślę, że mnie okłamałeś\n",
    "\n",
    "Powinien zwrócić zdanie\n",
    "> I think you lied to me.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Do7k4lf0zsxu"
   },
   "source": [
    "# Importy i przygotowanie danych\n",
    "Poniżej znajdują się importy bibliotek potrzebnych do rozwiązania problemu a także skrypt do ładowania zbioru danych zawierającego pary zdań w języku polskim i angielskim. Poniższy kod można odpalić i schować, ale zachęcamy do zaznajomienia się z tym jak wygląda obróbka danych.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zCAgsgj3d7Kf"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z1yFSn0ld7Ki"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4080'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KnEaLlNpire"
   },
   "source": [
    "Poniżej przygotowujemy klasę `Lang` która będzie służyła jako struktura do obsługiwania naszego języka (osobna dla angielskiego i polskiego w naszym przypadku). Do każdego słowa w języku przypisujemy indeks (liczbę porządkową identyfikującą słowo). Dodatkowo definiujemy trzy dodatkowe indeksy:\n",
    "\n",
    "* 0 dla początku zdania (Start of Sentence, SOS)\n",
    "* 1 dla końca zdania (End of Sentence, EOS)\n",
    "* 2 dla paddingu (\"pustych\" wartości). Wartościami tymi będziemy wypełniać zdania w batchu tak, żeby wszystkie były równej długości -- dzięki temu łatwiej będzie zrównoleglić przetwarzanie ich na GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "E3DtzxXTd7Kk"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
    "        self.n_words = 3 # Count SOS, EOS and PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrV27D5Xryh1"
   },
   "source": [
    "Funkcje do normalizowania wchodzących zdań - zamieniamy Unicode na ASCII, zamieniamy wszystkie wielkie litery na małe itd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JLLuOE80d7Kk"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = s.replace(\"ł\", \"l\")\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('eng-pol.txt', encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    lines = lines[1:]  # Skip first line with attributions.\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[1::2]] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ld3aJ9Nd7Kl"
   },
   "source": [
    "Wyrzućmy zdania które są zbyt długie (ponad 20 słów)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E6fbLbiQd7Km"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) <= MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) <= MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csmhl8Iod7Km"
   },
   "source": [
    "Pełny proces przetwarzania danych wygląda następująco:\n",
    "\n",
    "- Wczytujemy plik z danymi, dzielimy go na pary zdań.\n",
    "- Normalizujemy tekst\n",
    "- Zamieniamy zdania w listy słów.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRd-fNMAd7Km",
    "outputId": "4014a77d-a69b-4a12-81a0-80061ad3982c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "['sprobujmy cos .', 'let s try something .']\n",
      "Read 59749 sentence pairs\n",
      "Trimmed to 59404 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 29246\n",
      "pol 11953\n",
      "Przykładowe pary zdań:\n",
      "['poprawa wymaga zmian bycie doskonalym wymaga czestych zmian .', 'to improve is to change to be perfect is to change often .']\n",
      "['tom przebiegl sto metrow w dwanascie sekund .', 'tom ran a hundred meters in twelve seconds .']\n",
      "['nie czekalem dlugo zanim przyszedl .', 'i hadn t waited long before he came along .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(pairs[0])\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('pol', 'eng', True)\n",
    "print(\"Przykładowe pary zdań:\")\n",
    "for _ in range(3):\n",
    "    print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQc1t2ArshOa"
   },
   "source": [
    "Na koniec definiujemy jeszcze funkcje, które pozwolą nam zamienić zdania w tensory, które nasza sieć będzie w stanie zrozumieć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "leinIgjGd7Kq"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "def pad_sequences(data_batch):\n",
    "    pl_batch, en_batch = [], []\n",
    "    for pl_sentence, en_sentence in data_batch:\n",
    "        pl_batch += [pl_sentence]\n",
    "        en_batch += [en_sentence]\n",
    "    pl_batch = pad_sequence(pl_batch, padding_value=PAD_token, batch_first=True)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_token, batch_first=True)\n",
    "    return pl_batch, en_batch\n",
    "\n",
    "def prepare_dataset(batch_size):\n",
    "    rng = np.random.RandomState(567)\n",
    "    indices = np.arange(len(pairs))\n",
    "    rng.shuffle(indices)\n",
    "    train_indices = indices[:int(len(pairs) * 0.8)]\n",
    "    test_indices = indices[int(len(pairs) * 0.8):]\n",
    "    train_pairs = list(pairs[idx] for idx in train_indices)\n",
    "    test_pairs = list(pairs[idx] for idx in test_indices)\n",
    "    tensor_train_pairs = [tensorsFromPair(pairs[idx]) for idx in train_indices]\n",
    "    tensor_test_pairs = [tensorsFromPair(pairs[idx]) for idx in test_indices]\n",
    "    reference_translation = test_pairs\n",
    "\n",
    "    # Output in natural language?\n",
    "\n",
    "    train_loader = DataLoader(tensor_train_pairs, batch_size=batch_size,\n",
    "                            shuffle=True, collate_fn=pad_sequences)\n",
    "    test_loader = DataLoader(tensor_test_pairs, batch_size=batch_size,\n",
    "                            shuffle=True, collate_fn=pad_sequences)\n",
    "    return train_pairs, test_pairs, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf844auxd7Km"
   },
   "source": [
    "# Model Seq2Seq\n",
    "\n",
    "\n",
    "\n",
    "W tym celu wykorzystany rekurencyjne sieci neuronowe (RNN-y), które poznaliśmy na poprzednich zajęciach. Konkretniej zbudujemy za ich pomocą model Sequence to Sequence (Seq2Seq), w której wykorzystamy dwie sieci rekurencyjne:\n",
    "1. Enkoder, który będzie przyjmował kolejno słowa ze zdania wejściowego i kompresował informacje o nich w swoim stanie ukrytym.\n",
    "2. Dekoder, który będzie generował kolejne słowa w języku docelowym. \n",
    "\n",
    "![seq2seq](https://docs.chainer.org/en/stable/_images/seq2seq.png)\n",
    "Źródło: https://docs.chainer.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxaO-7_LtPUk"
   },
   "source": [
    "## Funkcje pomocnicze i ewaluacyjne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "04Zm6_MRd7Kq"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "def predict(encoder, decoder, inputs, targets=None, max_len=MAX_LENGTH):\n",
    "    batch_size = inputs.size(0)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(inputs)\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]] * batch_size, device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_output, decoder_attention = decoder(\n",
    "        decoder_input,\n",
    "        decoder_hidden,\n",
    "        targets=targets,\n",
    "        max_len=max_len,\n",
    "        encoder_outputs=encoder_outputs)\n",
    "    return decoder_output, decoder_attention\n",
    "\n",
    "def translate(encoder, decoder, sentence, show_attention=True):\n",
    "    inputs = tensorFromSentence(input_lang, sentence).unsqueeze(0).cuda()\n",
    "    decoder_output, decoder_attention = predict(encoder, decoder, inputs)\n",
    "\n",
    "    decoded_words = []\n",
    "    for word in decoder_output[0]:\n",
    "        top_word = word.argmax(-1).item()\n",
    "        decoded_words.append(output_lang.index2word[top_word])\n",
    "        if top_word == EOS_token:\n",
    "            break\n",
    "\n",
    "    if decoder_attention is not None and show_attention:\n",
    "        # [out_words, in_words]\n",
    "        att = decoder_attention.cpu().detach().numpy()\n",
    "        att = att[0, :len(decoded_words), :]\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        im = ax.imshow(att, vmin=0, vmax=1)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.set_xticklabels([''] + sentence.split(' ') +\n",
    "                        ['EOS'], rotation=90)\n",
    "        ax.set_yticklabels([''] + decoded_words)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "        \n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "def batch_translate(encoder, decoder, batch):\n",
    "    decoder_output, decoder_attention = predict(encoder, decoder, batch)\n",
    "\n",
    "    predicted_sentences = []\n",
    "\n",
    "    # TODO: potentially paralellize?\n",
    "    for batch_idx in range(len(batch)):\n",
    "        predicted_words = []\n",
    "        for word in decoder_output[batch_idx]:\n",
    "            top_word = word.argmax(-1).item()\n",
    "            if top_word == EOS_token:\n",
    "                break\n",
    "            predicted_words.append(output_lang.index2word[top_word])\n",
    "\n",
    "        predicted_sentences.append(predicted_words)\n",
    "\n",
    "    return predicted_sentences\n",
    "\n",
    "def dataset_translate(encoder, decoder, loader):\n",
    "    predicted_sentences = []\n",
    "    reference_sentences = [] \n",
    "    for batch_in, batch_out in loader:\n",
    "        translated = batch_translate(encoder, decoder, batch_in)\n",
    "        predicted_sentences.extend(translated)\n",
    "\n",
    "        # TODO: move to a separate file?\n",
    "        reference_words = []\n",
    "        for sentence_idx, sentence in enumerate(batch_out):\n",
    "            decoded_sentence = []\n",
    "            for word in sentence:\n",
    "                if word.item() == EOS_token:\n",
    "                    break\n",
    "                decoded_sentence.append(output_lang.index2word[word.item()])\n",
    "            reference_sentences.append(decoded_sentence)\n",
    "    \n",
    "    return predicted_sentences, reference_sentences\n",
    "\n",
    "\n",
    "def translate_randomly(encoder, decoder, pairs, n=10):\n",
    "    # TODO: reuse translate_given_pairs\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = translate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "\n",
    "def translate_given_pairs(encoder, decoder, pairs):\n",
    "    for pair in pairs:\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = translate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "        \n",
    "def plot_results(bleus, losses):\n",
    "    sns.set_style('whitegrid')\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "    axes[0].plot(np.arange(len(bleus)), bleus)\n",
    "    axes[0].set_xlabel(\"Epoka\")\n",
    "    axes[0].set_ylabel(\"BLEU\")\n",
    "    axes[1].plot(np.arange(len(losses)), losses)\n",
    "    axes[1].set_xlabel(\"Epoka\")\n",
    "    axes[1].set_ylabel(\"Koszt na zbiorze treningowym\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKEO71h8_mvq"
   },
   "source": [
    "## Pętla trenująca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "v2hJ24pEd7Kq"
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, lr=0.01, batch_size=256, teacher_forcing_ratio=0.5, epochs_num=100, clipping=1.0):\n",
    "\n",
    "    # Prepare dataset, loss functions, optimizer\n",
    "    train_pairs, test_pairs, train_loader, test_loader = prepare_dataset(batch_size)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "    bleus = []\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(epochs_num + 1):\n",
    "\n",
    "        # Training\n",
    "        epoch_train_loss = 0.\n",
    "        for in_batch, out_batch in train_loader:\n",
    "            in_batch, out_batch = in_batch.cuda(), out_batch.cuda()\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "        \n",
    "            teacher_inputs = out_batch if random.random() < teacher_forcing_ratio else None\n",
    "        \n",
    "            decoder_output, decoded_attention = predict(\n",
    "                encoder, decoder, in_batch,\n",
    "                targets=teacher_inputs,\n",
    "                max_len=out_batch.size(1)\n",
    "            )\n",
    "            print(out_batch.shape, decoder_output.shape)\n",
    "            #loss = criterion(decoder_output.transpose(1, 2), out_batch)\n",
    "            loss = criterion(decoder_output, out_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(encoder.parameters(), clipping)\n",
    "            nn.utils.clip_grad_norm_(decoder.parameters(), clipping)\n",
    "\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        if epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                print(\"=\" * 25, \"Translation test\", \"=\" * 25)\n",
    "                translate_randomly(encoder, decoder, test_pairs, n=5)\n",
    "\n",
    "            pred_sentences, ref_sentences = dataset_translate(encoder, decoder, test_loader)\n",
    "            bleu_val = bleu_score(pred_sentences, [[sentence] for sentence in ref_sentences])\n",
    "            print(\"=\" * 25, f\"BLEU: {bleu_val}\", \"=\" * 25)\n",
    "            bleus += [bleu_val]\n",
    "\n",
    "        mean_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses += [mean_train_loss]\n",
    "        print(f\"Epoch: {epoch}. Train loss: {mean_train_loss}\")\n",
    "    return bleus, train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkvoS5hCjgI2"
   },
   "source": [
    "<h4> Zadanie 1 - Dekoder w Seq2Seq </h4>\n",
    "\n",
    "W tym zadaniu należy zaimplementować dekoder z modelu Seq2Seq. Kod enkodera jest dostępny poniżej i ma Państwu ułatwić odpowiednie zaimplementowanie dekodera.\n",
    "\n",
    "Dekoder otrzymuje na wejściu następujące argumenty:\n",
    "- `input` - tensor o wymiarach `[batch_size, 1]` zawierający słowo `<BOS>`. Powinno być podane w pierwszym kroku wykonywania dekodera.\n",
    "- `hidden` - ostatnia reprezentacja ukryta z enkodera .\n",
    "- `targets` - `None` albo `torch.tensor` o wymiarach `[batch_size, seq_len]` zawierający indeksy słów w języku docelowym. Jeżeli jest podany to należy zaimplementować teacher forcing na jego podstawie.\n",
    "- `max_len` - Długość sekwencji, którą mamy zwrócić.\n",
    "- `encoder_outputs` - w tym zadaniu ten argument należy zignorować, przyda się dopiero w kolejnym zadaniu.\n",
    "\n",
    "Dekoder ma zwrócić dwie zmienne:\n",
    "- `output` - tensor o wymiarach `[batch_size, max_len, vocab_size]` reprezentujące logity, które po zaaplikowaniu softmaksa (co będzie zrobione już poza dekoderem) będą reprezentowały prawdopodobieństwa słów przewidzianych przez nasz dekoder.\n",
    "- `attention_weights` - w tym zadaniu należy zawsze zwracać `None`.\n",
    "\n",
    "Architektura głowy klasyfikacyjnej jest dowolna, natomiast zalecamy sieć z jedną warstwą ukrytą: `[hidden_size, hidden_size, vocab_size]` i aktywacją tanh.\n",
    "\n",
    "\n",
    "**HINT 1**: Warto pamiętać o argumencie `batch_first=True` przy definiowaniu RNN-a.\n",
    "\n",
    "**HINT 2**: W enkoderze mogliśmy użyć jednego wywołania klasy GRU, jako że od razu mieliśmy wszystkie wejścia (słowa języka wejściowego). W przypadku dekodera nie jest to możliwe, jako że wejściem w kroku `t+1` jest wyjście z kroku `t`. Oznacza to że prawdopodobnie potrzebna będzie pętla `for`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "I6FSVbB0d7Kn"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_cell = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.rnn_cell(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "MiBbM1IVd7Ko"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(vocab_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input, hidden, targets=None, max_len=None, encoder_outputs=None):\n",
    "        input = self.embedding(input)\n",
    "        for t in range(max_len):\n",
    "            if targets is not None:\n",
    "            # teacher forcing\n",
    "                input = targets[:, t] if random.random() < 0.5 else output.argmax(dim=1)\n",
    "            else:\n",
    "                input = self.embedding(input)\n",
    "            outputs, hidden = self.rnn(input, hidden)\n",
    "            \n",
    "            output = self.fc(outputs)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4KiePQdpivCd",
    "outputId": "5b1c6a1f-5035-4e83-f964-ed1a03e7a5d1",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'output' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [126], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m encoder \u001b[38;5;241m=\u001b[39m EncoderRNN(input_lang\u001b[38;5;241m.\u001b[39mn_words, embedding_size, hidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m decoder \u001b[38;5;241m=\u001b[39m DecoderRNN(output_lang\u001b[38;5;241m.\u001b[39mn_words, embedding_size, hidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m bleus, losses \u001b[38;5;241m=\u001b[39m train(encoder, decoder, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, lr\u001b[38;5;241m=\u001b[39mlr, epochs_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, clipping\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      9\u001b[0m plot_results(bleus, losses)\n",
      "Cell \u001b[0;32mIn [101], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, lr, batch_size, teacher_forcing_ratio, epochs_num, clipping)\u001b[0m\n\u001b[1;32m     25\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m teacher_inputs \u001b[38;5;241m=\u001b[39m out_batch \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m teacher_forcing_ratio \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m decoder_output, decoded_attention \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(out_batch\u001b[38;5;241m.\u001b[39mshape, decoder_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#loss = criterion(decoder_output.transpose(1, 2), out_batch)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [100], line 11\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(encoder, decoder, inputs, targets, max_len)\u001b[0m\n\u001b[1;32m      9\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[SOS_token]] \u001b[38;5;241m*\u001b[39m batch_size, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     10\u001b[0m decoder_hidden \u001b[38;5;241m=\u001b[39m encoder_hidden\n\u001b[0;32m---> 11\u001b[0m decoder_output, decoder_attention \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoder_output, decoder_attention\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [125], line 18\u001b[0m, in \u001b[0;36mDecoderRNN.forward\u001b[0;34m(self, input, hidden, targets, max_len, encoder_outputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# teacher forcing\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m targets[:, t] \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'output' referenced before assignment"
     ]
    }
   ],
   "source": [
    "hidden_size = 1024\n",
    "embedding_size = 512\n",
    "lr = 1e-3\n",
    "#torch.backends.cudnn.enabled = False\n",
    "encoder = EncoderRNN(input_lang.n_words, embedding_size, hidden_size).to(device)\n",
    "decoder = DecoderRNN(output_lang.n_words, embedding_size, hidden_size).to(device)\n",
    "\n",
    "bleus, losses = train(encoder, decoder, batch_size=512, lr=lr, epochs_num=10, clipping=0.1)\n",
    "plot_results(bleus, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crrwsvmgGHix"
   },
   "source": [
    "<h4> Zadanie 2 - Atencja w RNN-ach </h4>\n",
    "\n",
    "![seq2seq z dekoderem](https://www.researchgate.net/profile/Chandan_Reddy6/publication/329464533/figure/fig3/AS:701043021197314@1544153089772/An-attention-based-seq2seq-model.ppm)\n",
    "\n",
    "Źródło: https://github.com/google/seq2seq\n",
    "\n",
    "W tym zadaniu należy napisać kod nowego dekodera, który ma działać podobnie jak dekoder w poprzednim zadaniu, ale jednocześnie ma wykorzystywać mechanizm atencji.\n",
    "\n",
    "W normalnym dekoderze, w kroku `t` wejściem do komórki GRU (pomijamy tutaj przekazywanie stanu ukrytego) była wyłącznie zembeddowana reprezentacja $\\bar{y}_t$. W dekoderze z atencją na wejściu podawna będzie konkatenacja tego wektora oraz specjalnego wektora $z_t$ stworzonego na podstawie wyjść z enkodera: $\\tilde{h}_t = [\\bar{y}_t, z_t]$. \n",
    "\n",
    "Wektor $z_t$ jest pozyskiwany za pomocą mechanizmu atencji. Intuicyjnie chcielibyśmy w nim zebrać informacje z enkodera, które będą najistotniejsze przy dekodowaniu aktualnego słowa. Przyjmijmy, że mamy funkcję alignmentu $a(h, e)$, która jest nam w stanie powiedzieć jak bardzo podobne do siebie są stan ukryty dekodera $h$ oraz reprezentacja słowa $e$.\n",
    "\n",
    "Wtedy \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_i &= \\frac{ \\exp(a(h, e_i)) }{\\sum_{j} \\exp(a(h, e_j))} \\\\\n",
    "z_t &= \\sum_i e_i \\cdot w_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "W naszym wypadku funkcja alignmentu $a(h, e)$ ma być siecią neuronową z dwoma warstwami o kolejnych wymiarach: `[2 * hidden_size, hidden_size, 1]` i aktywacją tanh po pierwszej warstwie.\n",
    "\n",
    "Argumenty wejściowe i wyjściowe z dekodera są takie same jak w poprzednim z zadaniu z wyjątkiem:\n",
    "- Tym razem na wejściu otrzymujemy tensor `encoder_outputs` o wymiarach `[batch_size, encoder_seq_len, hidden_size]`. To są reprezentacje $e_i$, które należy wykorzystać w mechanizmie atencji.\n",
    "- Tym razem na wyjściu `attention_weights` powinno być tensorem o wymiarach `[batch_size, decoder_seq_len, encoder_seq_len]` zawierającym wagi $w_i$. **HINT:** wartości tego tensora powinny się sumować do jedynki na ostatnim wymiarze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "5bETuU9Hd7Kp"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "      def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.gru = nn.GRU(input_size=embedding_size+hidden_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.attention = nn.Sequential(\n",
    "          nn.Linear(2*hidden_size, hidden_size),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "      def forward(self, input, hidden, targets=None, max_len=None, encoder_outputs=None):\n",
    "        attention_weights = self.attention(torch.cat((input, hidden), dim=2))\n",
    "        attention_weights = attention_weights.squeeze(2)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        context = attention_weights.unsqueeze(2) * encoder_outputs\n",
    "        context = context.sum(dim=1)\n",
    "        input = self.embedding(input)\n",
    "        input = torch.cat((input, context), dim=2)\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mipeLXX9fx_7",
    "outputId": "29660cba-a8e9-4460-d45d-712d2ac4de89",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'attention_weights' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [136], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m encoder \u001b[38;5;241m=\u001b[39m EncoderRNN(input_lang\u001b[38;5;241m.\u001b[39mn_words, embedding_size, hidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m decoder \u001b[38;5;241m=\u001b[39m AttnDecoderRNN(output_lang\u001b[38;5;241m.\u001b[39mn_words, embedding_size, hidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m bleus, losses \u001b[38;5;241m=\u001b[39m train(encoder, decoder, lr\u001b[38;5;241m=\u001b[39mlr, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, epochs_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, clipping\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      9\u001b[0m plot_results(bleus, losses)\n",
      "Cell \u001b[0;32mIn [101], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, lr, batch_size, teacher_forcing_ratio, epochs_num, clipping)\u001b[0m\n\u001b[1;32m     25\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m teacher_inputs \u001b[38;5;241m=\u001b[39m out_batch \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m teacher_forcing_ratio \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m decoder_output, decoded_attention \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(out_batch\u001b[38;5;241m.\u001b[39mshape, decoder_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#loss = criterion(decoder_output.transpose(1, 2), out_batch)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [100], line 11\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(encoder, decoder, inputs, targets, max_len)\u001b[0m\n\u001b[1;32m      9\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[SOS_token]] \u001b[38;5;241m*\u001b[39m batch_size, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     10\u001b[0m decoder_hidden \u001b[38;5;241m=\u001b[39m encoder_hidden\n\u001b[0;32m---> 11\u001b[0m decoder_output, decoder_attention \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoder_output, decoder_attention\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [135], line 18\u001b[0m, in \u001b[0;36mAttnDecoderRNN.forward\u001b[0;34m(self, input, hidden, targets, max_len, encoder_outputs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, hidden, targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, encoder_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     17\u001b[0m   \u001b[38;5;66;03m#attention_weights = self.attention(torch.cat((input, hidden), dim=2))\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m   attention_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattention_weights\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     19\u001b[0m   attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attention_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m   context \u001b[38;5;241m=\u001b[39m attention_weights\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m encoder_outputs\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'attention_weights' referenced before assignment"
     ]
    }
   ],
   "source": [
    "hidden_size = 1024\n",
    "embedding_size = 512\n",
    "lr = 1e-3\n",
    "clip = 0.1\n",
    "encoder = EncoderRNN(input_lang.n_words, embedding_size, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(output_lang.n_words, embedding_size, hidden_size).to(device)\n",
    "\n",
    "bleus, losses = train(encoder, decoder, lr=lr, batch_size=512, epochs_num=10, clipping=0.1)\n",
    "plot_results(bleus, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPmEeponfgob"
   },
   "source": [
    "# Powiązana literatura\n",
    "\n",
    "* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "\n",
    "## Przydatne tutoriale\n",
    "\n",
    "* https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "* https://github.com/bentrevett/pytorch-seq2seq\n",
    "* https://github.com/gmum/AppliedDL2020/tree/master/Week%207 - materiały z kursu Applied Deep Learning prowadzonego w semestrze letnim\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "12_seq2seq_SOLVED_BOOK.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
